# ENOVA Prometheus Alerting Rules
# Golden Signals: Latency, Traffic, Errors, Saturation
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: enova-golden-signals
  namespace: monitoring
  labels:
    app: enova
    prometheus: main
spec:
  groups:
    # ═══════════════════════════════════════════════════════════════
    # LATENCY ALERTS
    # ═══════════════════════════════════════════════════════════════
    - name: enova.latency
      rules:
        - alert: HighAPILatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{service="api-gateway"}[5m]))
              by (le)
            ) > 0.5
          for: 5m
          labels:
            severity: warning
            service: api-gateway
          annotations:
            summary: "High API latency detected"
            description: "API Gateway P95 latency is {{ $value | humanizeDuration }} (threshold: 500ms)"
            runbook_url: "https://docs.enova.com/runbooks/high-latency"

        - alert: CriticalChatLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(ws_message_duration_seconds_bucket{service="chat-service"}[2m]))
              by (le)
            ) > 0.1
          for: 2m
          labels:
            severity: critical
            service: chat-service
          annotations:
            summary: "Chat latency exceeds 100ms"
            description: "Chat P95 latency is {{ $value | humanizeDuration }} - users will notice lag"

    # ═══════════════════════════════════════════════════════════════
    # TRAFFIC ALERTS
    # ═══════════════════════════════════════════════════════════════
    - name: enova.traffic
      rules:
        - alert: TrafficSpike
          expr: |
            sum(rate(http_requests_total{service="api-gateway"}[5m]))
            > 2 * avg_over_time(sum(rate(http_requests_total{service="api-gateway"}[5m]))[1h:5m])
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traffic spike detected"
            description: "Request rate is 2x higher than average"

        - alert: TrafficDrop
          expr: |
            sum(rate(http_requests_total{service="api-gateway"}[5m]))
            < 0.5 * avg_over_time(sum(rate(http_requests_total{service="api-gateway"}[5m]))[1h:5m])
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Unusual traffic drop"
            description: "Request rate dropped to 50% of average - possible outage"

    # ═══════════════════════════════════════════════════════════════
    # ERROR ALERTS
    # ═══════════════════════════════════════════════════════════════
    - name: enova.errors
      rules:
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            / sum(rate(http_requests_total[5m])) > 0.01
          for: 5m
          labels:
            severity: critical
            action: rollback
          annotations:
            summary: "Error rate exceeds 1%"
            description: "{{ $value | humanizePercentage }} of requests are failing"
            runbook_url: "https://docs.enova.com/runbooks/high-error-rate"

        - alert: JWTValidationErrors
          expr: |
            sum(rate(auth_jwt_validation_errors_total[5m])) > 10
          for: 2m
          labels:
            severity: warning
            service: api-gateway
          annotations:
            summary: "JWT validation errors spiking"
            description: "{{ $value }} JWT errors per second - possible attack or config issue"

        - alert: WebSocketDisconnections
          expr: |
            sum(rate(ws_disconnections_total{service="chat-service"}[5m])) > 50
          for: 5m
          labels:
            severity: warning
            service: chat-service
          annotations:
            summary: "High WebSocket disconnection rate"
            description: "{{ $value }} disconnections/sec - users experiencing chat instability"

    # ═══════════════════════════════════════════════════════════════
    # SATURATION ALERTS
    # ═══════════════════════════════════════════════════════════════
    - name: enova.saturation
      rules:
        - alert: HighCPUUsage
          expr: |
            sum(rate(container_cpu_usage_seconds_total{namespace="enova-staging"}[5m]))
            by (pod)
            / sum(container_spec_cpu_quota{namespace="enova-staging"}/container_spec_cpu_period{namespace="enova-staging"})
            by (pod) > 0.8
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod CPU usage high"
            description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} CPU"

        - alert: HighMemoryUsage
          expr: |
            container_memory_usage_bytes{namespace="enova-staging"}
            / container_spec_memory_limit_bytes{namespace="enova-staging"} > 0.85
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod memory usage high"
            description: "Pod {{ $labels.pod }} memory at {{ $value | humanizePercentage }}"

        - alert: PodRestartLoop
          expr: |
            increase(kube_pod_container_status_restarts_total{namespace="enova-staging"}[15m]) > 3
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Pod restarting frequently"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in 15 minutes"

    # ═══════════════════════════════════════════════════════════════
    # CANARY SPECIFIC ALERTS
    # ═══════════════════════════════════════════════════════════════
    - name: enova.canary
      rules:
        - alert: CanaryErrorRateHigh
          expr: |
            sum(rate(http_requests_total{status=~"5..", canary="true"}[5m]))
            / sum(rate(http_requests_total{canary="true"}[5m]))
            >
            2 * (sum(rate(http_requests_total{status=~"5..", canary="false"}[5m]))
            / sum(rate(http_requests_total{canary="false"}[5m])))
          for: 5m
          labels:
            severity: critical
            action: rollback
          annotations:
            summary: "Canary error rate is 2x higher than production"
            description: "IMMEDIATE ROLLBACK RECOMMENDED - Canary showing {{ $value | humanizePercentage }} error rate"

        - alert: CanaryLatencyHigh
          expr: |
            histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{canary="true"}[5m])) by (le))
            >
            1.5 * histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{canary="false"}[5m])) by (le))
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Canary latency 50% higher than production"
            description: "Consider pausing canary rollout until investigated"
